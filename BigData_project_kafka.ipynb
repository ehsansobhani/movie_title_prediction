{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f9fe72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace ,col, when\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, StopWordsRemover, Tokenizer\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "import random\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.conf import SparkConf\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "from json import loads\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b47eb080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# consumer = KafkaConsumer('Ehsan',bootstrap_servers=['localhost:9092'],consumer_timeout_ms = 20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fd583c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Kafka consumer\n",
    "consumer = KafkaConsumer(\n",
    "    'Ehsan',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),consumer_timeout_ms = 20000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "7ab6ef93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Spark session\n",
    "conf = SparkConf().setAppName(\"Title_suggestion\").set('spark.executor.memory', '4g').set('spark.shuffle.service.enabled', 'false').set('spark.dynamicAllocation.enabled','false')\n",
    "\n",
    "spark = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f1afdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the movie abstract from the user\n",
    "# abstract = input(\"Please enter the movie abstract: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b7feaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the summaries, titles, and ratings\n",
    "summaries = []\n",
    "titl = []\n",
    "ratin = []\n",
    "\n",
    "# Consume messages from Kafka\n",
    "for message in consumer:\n",
    "    # Parse the message value as JSON\n",
    "    movie = message.value\n",
    "\n",
    "    # Append the summary, title, and rating to the respective lists\n",
    "    summaries.append(movie['summary'])\n",
    "    titl.append(movie['title'])\n",
    "    ratin.append(movie['rating'])\n",
    "\n",
    "# Save the summaries in a variable\n",
    "abstract = ' '.join(summaries)\n",
    "\n",
    "# Save the titles and ratings in a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Title': titl,\n",
    "    'Rating': ratin\n",
    "})\n",
    "\n",
    "# Consume messages from Kafka\n",
    "for message in consumer:\n",
    "    # Parse the message value as JSON\n",
    "    movie = message.value\n",
    "\n",
    "    # Print the summary, title, and rating\n",
    "    print(f\"Title: {movie['title']}\")\n",
    "    print(f\"Rating: {movie['rating']}\")\n",
    "    print(f\"Summary: {movie['summary']}\")\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Close the consumer connection\n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4b314884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ajcjHBIBKUHKJADBNCK;JBKLJBKUBHUJBSCKHKBKJB']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2c4aa19f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratin[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "068e1ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Toy Story (1995)'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titl[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "68c0587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ajcjhbibkuhkjadbnckjbkljbkubhujbsckhkbkjb']\n"
     ]
    }
   ],
   "source": [
    "# Sample data (replace this with your text data)\n",
    "data = [(abstract,)]\n",
    "\n",
    "# Define schema and create a DataFrame\n",
    "schema = [\"text\"]\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "# Remove punctuation and symbols\n",
    "cleaned_data = df.withColumn(\"cleaned_text\", regexp_replace(\"text\", \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "# Tokenization - Convert cleaned text into tokens (words)\n",
    "tokenizer = Tokenizer(inputCol=\"cleaned_text\", outputCol=\"words\")\n",
    "words_data = tokenizer.transform(cleaned_data)\n",
    "\n",
    "# Remove stop words\n",
    "stop_words = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=stop_words)\n",
    "filtered_data = stopwords_remover.transform(words_data)\n",
    "\n",
    "# Get unique words after removing stop words\n",
    "words = filtered_data.select(\"filtered_words\").rdd.flatMap(lambda row: row.filtered_words).distinct().collect()\n",
    "\n",
    "# Display the words after removing punctuation, symbols, and stop words\n",
    "print(words)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "df971dcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|title                                    |\n",
      "+-----------------------------------------+\n",
      "|ajcjhbibkuhkjadbnckjbkljbkubhujbsckhkbkjb|\n",
      "|ajcjhbibkuhkjadbnckjbkljbkubhujbsckhkbkjb|\n",
      "|ajcjhbibkuhkjadbnckjbkljbkubhujbsckhkbkjb|\n",
      "|ajcjhbibkuhkjadbnckjbkljbkubhujbsckhkbkjb|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample list of words (replace this with your word list)\n",
    "# words = [\"adventure\", \"mystery\", \"fantasy\", \"thriller\", \"comedy\", \"drama\", \"action\"]\n",
    "\n",
    "# Generate titles\n",
    "titles = [' '.join(random.sample(words, min(2, len(words)))) for _ in range(4)]\n",
    "\n",
    "# Create a DataFrame from the generated titles\n",
    "title_df = spark.createDataFrame([(title,) for title in titles], [\"title\"])\n",
    "title_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5ade4a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = (\n",
    "    spark.read.csv(\n",
    "        path=\"E:/fall2023/5012- Big Data/project/data-sets/ratings.csv\",\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        quote='\"',\n",
    "        schema=\"userId INT, movieId INT, rating DOUBLE, timestamp INT\",\n",
    "    )\n",
    "    # .withColumn(\"timestamp\", f.to_timestamp(f.from_unixtime(\"timestamp\")))\n",
    "    .drop(\"timestamp\")\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31a051",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5fca901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = spark.read.csv(path=\"E:/fall2023/5012- Big Data/project/data-sets/movies.csv\",\n",
    "        sep=\",\",\n",
    "        header=True,\n",
    "        quote='\"',\n",
    "        schema=\"movieId INT, title STRING, genres STRING\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "75ad4ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the data on the movieId column\n",
    "data = movies.join(ratings, on=\"movieId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ca3da115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's say you want to update the 'summary' column to 'Excellent' when the 'rating' is '5'\n",
    "data = data.withColumn('rating', when(data['title'] == titl[0], ratin[0]).otherwise(data['rating']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9d542332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+------+------+\n",
      "|movieId|               title|              genres|userId|rating|\n",
      "+-------+--------------------+--------------------+------+------+\n",
      "|      1|    Toy Story (1995)|Adventure|Animati...|     1|     5|\n",
      "|      3|Grumpier Old Men ...|      Comedy|Romance|     1|   4.0|\n",
      "|      6|         Heat (1995)|Action|Crime|Thri...|     1|   4.0|\n",
      "|     47|Seven (a.k.a. Se7...|    Mystery|Thriller|     1|   5.0|\n",
      "|     50|Usual Suspects, T...|Crime|Mystery|Thr...|     1|   5.0|\n",
      "+-------+--------------------+--------------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f2049371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can split the data into training and test sets\n",
    "train_data, test_data = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09fc0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\")\n",
    "idf = IDF(inputCol=hashingTF.getOutputCol(), outputCol=\"features\")\n",
    "indexer = StringIndexer(inputCol=\"genres\", outputCol=\"genresIndex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "803bd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up regression model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"rating\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55947498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, idf, indexer, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6aa5f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "model = pipeline.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0949c353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2de56f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 0.997643\n"
     ]
    }
   ],
   "source": [
    "# Train model on the training data\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"rating\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"rmse\"\n",
    ")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a433a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "prediction = model.transform(title_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17217a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by the 'prediction' column in ascending order\n",
    "prediction_sorted = prediction.sort(col(\"prediction\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e5a6ea69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+\n",
      "|               title|      prediction|\n",
      "+--------------------+----------------+\n",
      "|   stringtype import|3.51051048612971|\n",
      "|   import structtype|3.51051048612971|\n",
      "|stringtype pyspar...|3.51051048612971|\n",
      "|structfield pyspa...|3.51051048612971|\n",
      "+--------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_sorted.select(\"title\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc32f58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://docs.google.com/spreadsheets/d/random_link\"\n",
    "import pandas as pd\n",
    "df=spark.createDataFrame(pd.read_csv(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56af1bb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54b3a6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Subscribe to 1 topic\u001b[39;00m\n\u001b[0;32m      2\u001b[0m df \u001b[38;5;241m=\u001b[39m spark \\\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;241m.\u001b[39mreadStream \\\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkafka.bootstrap.servers\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhost1:port1,host2:port2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msubscribe\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtopic1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m----> 7\u001b[0m   \u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      8\u001b[0m df\u001b[38;5;241m.\u001b[39mselectExpr(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(key AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCAST(value AS STRING)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\pyspark\\sql\\streaming\\readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload())\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide."
     ]
    }
   ],
   "source": [
    "# Subscribe to 1 topic\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"host1:port1,host2:port2\") \\\n",
    "  .option(\"subscribe\", \"topic1\") \\\n",
    "  .load()\n",
    "df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988ef2ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark.streaming.kafka'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StreamingContext\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#    Kafka\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstreaming\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkafka\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KafkaUtils\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#    json parsing\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark.streaming.kafka'"
     ]
    }
   ],
   "source": [
    "#    Spark\n",
    "from pyspark import SparkContext\n",
    "#    Spark Streaming\n",
    "from pyspark.streaming import StreamingContext\n",
    "#    Kafka\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "#    json parsing\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837c9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2df399",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "from time import sleep\n",
    "import json\n",
    "from datetime import datetime\n",
    "from kafka import KafkaConsumer\n",
    "from json import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0d2672",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer('Ehsan',bootstrap_servers=['localhost:9092'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ebb20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for message in consumer:\n",
    "    print(message.value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
